## 原因

1. 忘记对你的数据进行归一化

2. 忘记检查输出结果

3. 没有对数据进行预处理

4. 没有使用任何的正则化方法

5. 使用了一个太大的 batch size

6. 使用一个错误的学习率

7. 在最后一层使用错误的激活函数

8. 网络包含坏的梯度

9. 网络权重没有正确的初始化

10. 使用了一个太深的神经网络

11. 隐藏层神经元数量设置不正确

对应的解决办法分别是：

1. 对数据进行归一化，常用的归一化包括**零均值归一化**和**线性函数归一化**方法；
2. 检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；
3. 对数据进行预处理，包括做一些简单的转换；
4. 采用正则化方法，比如 L2 正则，或者 dropout；
5. 在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。
6. **不采用梯度裁剪**。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。
7. 如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；
8. 如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。
9. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
10. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
11. 从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字